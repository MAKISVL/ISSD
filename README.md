# ИС сбора данных, определяемых пользователем

![Python](https://img.shields.io/badge/Python-3.11-blue.svg?style=for-the-badge&logo=python)
![Flask](https://img.shields.io/badge/Flask-Framework-black.svg?style=for-the-badge&logo=flask)
![Docker](https://img.shields.io/badge/Docker-Containerization-blue.svg?style=for-the-badge&logo=docker)
![Nginx](https://img.shields.io/badge/Nginx-Webserver-green.svg?style=for-the-badge&logo=nginx)
![BeautifulSoup](https://img.shields.io/badge/BeautifulSoup-HTML_Parsing-orange.svg?style=for-the-badge&logo=html5)
![Selenium](https://img.shields.io/badge/Selenium-Web_Automation-lightgray.svg?style=for-the-badge&logo=selenium)

Информационная система для сбора данных с веб-сайтов, позволяющая пользователю определять необходимые данные для извлечения, с поддержкой различных методов парсинга и обхода блокировок.

---

## О проекте

Этот веб-парсер представляет собой **Flask-приложение**, разработанное для упрощения процесса сбора данных с веб-страниц. Пользователи могут указать URL-адрес и выбрать интересующие их типы данных (заголовки, ссылки, изображения, цены, текст, мета-теги Open Graph). Система также поддерживает **ручной ввод CSS-селекторов и XPath** для более точного сбора данных, а также включает продвинутые методы обхода блокировок.

## Функционал

* **Автоматический парсинг:** Сбор основных элементов страницы (заголовки h1-h6, ссылки, изображения, параграфы текста, мета-описание, цены).
* **Ручной парсинг по селекторам:** Возможность задавать пользовательские CSS-селекторы и XPath для извлечения специфических данных.
* **Сбор Open Graph мета-тегов:** Извлечение информации для социальных сетей (`og:title`, `og:description`, `og:image` и т.д.).
* **Загрузка данных:** Экспорт собранных данных в формате **CSV**.
* **Загрузка изображений:** Скачивание найденных изображений в **ZIP-архиве**.
* **Продвинутый обход блокировок:**
    * Автоматическая ротация **User-Agent**.
    * Добавление заголовков **Referer** для имитации перехода с поисковых систем.
    * Поддержка использования **прокси-серверов** (HTTP/HTTPS) через веб-интерфейс.
    * Опциональное использование **Selenium** для парсинга сайтов с динамическим контентом (JavaScript) и сложными блокировками.
* **Интуитивно понятный интерфейс:** Веб-интерфейс на Flask с использованием Bootstrap.
* **Контейнеризация:** Проект упакован в Docker-контейнеры для легкого развертывания и масштабирования.

## Используемые технологии

* **Backend:** Python 3.11, Flask
* **Парсинг:** BeautifulSoup4, Requests
* **Обход блокировок:** Selenium, Gunicorn (WSGI-сервер), Random User-Agents, HTTP Headers
* **Управление данными:** Pandas
* **Развертывание:** Docker, Docker Compose, Nginx (обратный прокси)

## Развертывание и запуск

Проект разработан для развертывания с использованием Docker Compose, что упрощает запуск как локально, так и на удаленном сервере.

### Локальный запуск (для разработки)

1.  **Клонируйте репозиторий:**
    ```bash
    git clone [https://github.com/MAKISVL/universal-parser-app.git](https://github.com/MAKISVL/universal-parser-app.git)
    cd universal-parser-app
    ```
2.  **Установите Docker и Docker Compose:**
    Убедитесь, что у вас установлены Docker Engine и Docker Compose (обычно поставляется с Docker Desktop).
3.  **Запустите приложение:**
    ```bash
    docker compose up --build -d
    ```
    (Первый запуск займет некоторое время, так как Docker будет скачивать базовые образы и устанавливать зависимости, включая Chromium для Selenium.)
4.  **Доступ к приложению:**
    Откройте веб-браузер и перейдите по адресу: `http://localhost/` или `http://127.0.0.1/`

### Развертывание на удаленном сервере (например, Ubuntu 24.04)

1.  **Подготовьте сервер:**
    * Подключитесь к серверу по SSH.
    * Установите Docker Engine и Docker Compose. (Если не уверены, выполните команды из шага 2 Локального запуска, добавив `sudo` при необходимости).
    * Рекомендуется добавить вашего пользователя в группу `docker` (`sudo usermod -aG docker ваш_пользователь` и переподключиться).

2.  **Клонируйте репозиторий на сервер:**
    ```bash
    cd /var/www/ # Или в другой выбранный вами каталог
    sudo git clone [https://github.com/MAKISVL/universal-parser-app.git](https://github.com/MAKISVL/universal-parser-app.git) . # Клонирует в текущую папку
    cd universal-parser-app/
    ```

3.  **Настройте DNS для вашего домена (`issdvlasov.com`):**
    * **У регистратора домена (`issdvlasov.com`):** Измените неймсерверы домена на те, что предоставил ваш DNS-хостинг (например, `ns1.the.hosting`, `ns2.the.hosting`). **Это КРИТИЧЕСКИ ВАЖНЫЙ шаг.**
    * **В панели управления вашего DNS-хостинга (`the.hosting`):** Убедитесь, что для `issdvlasov.com` (и `www.issdvlasov.com`) созданы A-записи, указывающие на публичный IP-адрес вашего сервера (`5.180.30.17`).
    * **Дождитесь распространения DNS** (от нескольких минут до нескольких часов).

4.  **Запустите приложение на сервере:**
    ```bash
    docker compose up --build -d
    ```
    (Если вы видите предупреждения об устаревшей версии в `docker-compose.yml`, просто удалите строку `version: '3.8'` из файла и повторите `git pull`, затем `docker compose up -d`.)

5.  **Настройте фаервол (если используете UFW):**
    ```bash
    sudo ufw allow 80/tcp # Для HTTP
    sudo ufw reload
    ```

6.  **Доступ к приложению:**
    После успешного запуска и распространения DNS, откройте веб-браузер и перейдите по адресу: `http://issdvlasov.com/` (или по IP-адресу сервера `http://5.180.30.17/`, если не хотите использовать домен).

7.  **Настройка HTTPS (рекомендуется):**
    Для безопасного доступа по `https://issdvlasov.com/` используйте Certbot:
    ```bash
    sudo apt install certbot python3-certbot-nginx -y
    sudo certbot --nginx -d issdvlasov.com -d [www.issdvlasov.com](https://www.issdvlasov.com)
    ```
    Следуйте инструкциям Certbot.

---

## Структура проекта
<img width="714" height="375" alt="image" src="https://github.com/user-attachments/assets/0a1638f3-2108-4afe-a7fb-b0cc5b616fd6" />
## Использование приложения

1.  Введите **URL-адрес** сайта, который вы хотите спарсить.
2.  Выберите необходимые **опции для сбора данных** (заголовки, ссылки, изображения, цены, описание, Open Graph мета-теги, строки текста).
3.  При необходимости укажите **прокси-сервер**.
4.  Для сложных сайтов или сайтов с динамическим контентом (JS) отметьте чекбокс "**Использовать Selenium**".
5.  Нажмите "**Начать сбор**".
6.  Результаты будут отображены в таблице, с возможностью скачать их в **CSV** или изображения в **ZIP-архиве**.
7.  Для более точного контроля используйте "**Ручной сбор по селекторам**", где можно ввести конкретные CSS-селекторы или XPath.

---

## Автор

**Власов Максим**, группа **БВТ2355**

---
